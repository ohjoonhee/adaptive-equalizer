from pathlib import Path
from typing import Union, Optional

from collections import namedtuple

import os
import os.path as osp
import glob

import librosa
import numpy as np

import torch
import torchaudio
from torch.utils.data import DataLoader, Dataset


import lightning as L
from lightning.pytorch.utilities.types import TRAIN_DATALOADERS, EVAL_DATALOADERS

from transforms.base import BaseTransforms
from utils.make_eqs import make_random_eq

path_item = namedtuple("path_item", ["audio_path", "eq_path"])


class FMAMiddle(Dataset):
    def __init__(
        self,
        root: str,
        subset: Optional[str] = None,
        val_eq_dir: str = "random_walk_eqs_db",
        eq_shape: int = 1025,  # FIXME: Hardcoded and type is limited to int
        sr: Optional[int] = None,
    ) -> None:
        super().__init__()
        self.root = root
        self.subset = subset
        self.eq_shape = eq_shape
        self.val_eq_dir = val_eq_dir
        self.sr = sr

        self.paths = self.load_data(subset)

    def load_data(self, subset: Optional[str] = None) -> list[path_item]:
        audio_paths = glob.glob(osp.join(self.root, "**/*.wav"))
        audio_paths = sorted(audio_paths)
        paths = []

        if subset == "training":
            audio_paths = audio_paths[:-5000]
            paths = [path_item(audio_path, None) for audio_path in audio_paths]

        elif subset == "validation":
            audio_paths = audio_paths[-5000:-2500]
            eq_path_root = osp.join(self.root, self.val_eq_dir)
            eq_paths = sorted(os.listdir(eq_path_root))
            eq_paths_gen = (osp.join(eq_path_root, f) for f in eq_paths)
            for audio_path in audio_paths:
                paths.append(path_item(audio_path, next(eq_paths_gen)))

        elif subset == "test":
            audio_paths = audio_paths[-2500:]
            eq_path_root = osp.join(self.root, self.val_eq_dir)
            eq_paths = sorted(os.listdir(eq_path_root))
            eq_paths_gen = (osp.join(eq_path_root, f) for f in eq_paths)
            for audio_path in audio_paths:
                paths.append(path_item(audio_path, next(eq_paths_gen)))
        else:
            raise ValueError("Invalid subset")

        print(f"Loaded {len(paths)} audio segments.")

        return paths

    def __getitem__(self, index):
        path = self.paths[index]

        # load audio
        clean_audio, sr = librosa.load(path.audio_path, sr=self.sr)

        # load eq and apply to spec
        if self.subset == "training":
            eq = make_random_eq(self.eq_shape).astype(np.float32)
        elif self.subset == "validation":
            eq = np.load(path.eq_path).astype(np.float32)
        elif self.subset == "test":
            # TODO: Implement test set
            pass

        return {
            "clean_audio": clean_audio,
            "label": eq,
        }

    def __len__(self):
        return len(self.paths)


class FMAMediumDataModule(L.LightningDataModule):
    def __init__(
        self,
        root: str,
        batch_size: int,
        # transforms: BaseTransforms,
        num_workers: int,
        audio_cache_dir: str = "segments_3sec",
        val_eq_dir: str = "random_walk_eqs",
        eq_shape: int = 1025,  # FIXME: Hardcoded and type is limited to int
        sr: Optional[int] = None,
    ) -> None:
        super().__init__()
        self.root = root
        # self.transforms = transforms
        # self.train_transform = transforms.train_transform()
        # self.val_transform = transforms.val_transform()
        # self.test_transform = transforms.test_transform()
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.audio_cache_dir = audio_cache_dir
        self.val_eq_dir = val_eq_dir
        self.eq_shape = eq_shape
        self.sr = sr

    def setup(self, stage: str) -> None:
        if stage in ["fit", "validate"]:
            self.train_dataset = FMAMiddle(
                self.root,
                subset="training",
                # transform=self.train_transform,
                # audio_cache_dir=self.audio_cache_dir,
                val_eq_dir=self.val_eq_dir,
                eq_shape=self.eq_shape,
                sr=self.sr,
            )
            self.val_dataset = FMAMiddle(
                self.root,
                subset="validation",
                # transform=self.val_transform,
                # audio_cache_dir=self.audio_cache_dir,
                val_eq_dir=self.val_eq_dir,
                eq_shape=self.eq_shape,
                sr=self.sr,
            )
        else:
            self.test_dataset = FMAMiddle(
                self.root,
                subset="testing",
                # transform=self.test_transform,
                # audio_cache_dir=self.audio_cache_dir,
                val_eq_dir=self.val_eq_dir,
                eq_shape=self.eq_shape,
                sr=self.sr,
            )

    def _collate_fn(self, batch):
        clean_audios = [item["clean_audio"] for item in batch]
        labels = torch.stack([torch.tensor(item["label"]) for item in batch])

        min_len = min([len(audio) for audio in clean_audios])
        clean_audios = torch.stack(
            [torch.tensor(audio[:min_len]) for audio in clean_audios]
        )

        batch = {
            "clean_audio": clean_audios,
            "label": labels,
        }

        return batch

    def train_dataloader(self) -> TRAIN_DATALOADERS:
        return DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=self.num_workers,
            collate_fn=self._collate_fn,
        )

    def val_dataloader(self) -> EVAL_DATALOADERS:
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            collate_fn=self._collate_fn,
        )

    def test_dataloader(self) -> EVAL_DATALOADERS:
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            # collate_fn=self._collate_fn,
        )
